/**
 * Copyright 2005 The Apache Software Foundation
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.nutch.admin.crawldb;

import java.io.File;
import java.io.IOException;
import java.util.HashSet;
import java.util.Iterator;
import java.util.LinkedList;
import java.util.List;
import java.util.Set;
import java.util.StringTokenizer;
import java.util.TreeMap;
import java.util.Map;
import java.util.logging.Logger;

import javax.servlet.http.HttpServlet;
import javax.servlet.http.HttpServletRequest;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.MapFile;
import org.apache.hadoop.io.SequenceFile;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.io.UTF8;
import org.apache.hadoop.io.MapWritable;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.io.MapFile.Reader;
import org.apache.hadoop.mapred.MapFileOutputFormat;
import org.apache.hadoop.mapred.SequenceFileInputFormat;
import org.apache.hadoop.mapred.SequenceFileOutputFormat;
import org.apache.hadoop.mapred.SequenceFileRecordReader;
import org.apache.nutch.admin.DefaultGuiComponent;
import org.apache.nutch.crawl.CrawlDatum;
import org.apache.nutch.crawl.CrawlDb;
import org.apache.nutch.util.NutchConfiguration;

public class oldUrlWithStatus {
    private static final Logger LOG = Logger.getLogger(UrlWithStatus.class.getName());
    
    public static final int PAGE_SIZE = 20;
    
    private static UrlWithStatus instance;
    private int currentPage ;
    
    private Configuration configuration;
    
    private class URL_with_data {
    	public String url;
    	public CrawlDatum crawlDatum;
    	URL_with_data(String u, CrawlDatum c){
    		url = u;
    		crawlDatum = c;
    	}
    }

    private UrlWithStatus() throws IOException {
    	crawlDb = _crawlDb;
    	configuration = _configuration;
    	reset();
    }
    
    public void init() {
        init(NutchConfiguration.create());
      }
      
      public void init(Configuration conf) {
        try {
          bean = NutchBean.get(this.getServletContext(), conf);
        } catch (IOException e) {
          // nothing
        }
      }
    
    public void reset() throws IOException {
        this.fReaderSet = new HashSet();
        FileSystem fs = FileSystem.get(configuration);
        if (fs.exists(crawlDb)) {
            MapFile.Reader[] readers = MapFileOutputFormat.getReaders( fs, new Path(crawlDb, CrawlDb.CURRENT_NAME), configuration );
            for (int i = 0; i < readers.length; i++) {
                Reader reader = readers[i];
                
            	LOG.info("Reading index and collecting sync marker positions...");
                int counter = 0;
            	Text url = new Text();
            	CrawlDatum cd = new CrawlDatum();
                do {
                	counter++;
                }while(reader.next(url, cd));
                LOG.info("Done seeking the file. (" + (counter-1) + " keys)");

                
                this.fReaderSet.add(reader);
            }
        }
        currentPage = 0;
    }
    
    public static UrlWithStatus getInstance( Path crawlDb, Configuration configuration, boolean reset) throws IOException {
        if (instance == null || reset) {
            instance = new UrlWithStatus(crawlDb, configuration);
        }
        return instance;
    }
    
    
    Iterator iterator;
    Reader reader;
    
    private URL_with_data getNext(boolean filterUrl, String urlFilter, boolean filterStatus, byte status)
    		throws InstantiationException, IllegalAccessException, IOException {
    	if (reader == null) {
    	   	if (iterator.hasNext()) {
    	   		reader = (Reader) iterator.next();
    	   	} else {
    	   		return null;
    	   	}
    	}
		WritableComparable key = (WritableComparable) reader.getKeyClass().newInstance();
		Writable value = (Writable) reader.getValueClass().newInstance();
    	
    	while(reader.next(key, value)) {
    		String url = key.toString();
    		CrawlDatum crawlDatum = (CrawlDatum) value;
    		if (filterStatus && crawlDatum.getStatus() != status) continue;
    		if (filterUrl && !url.contains(urlFilter)) continue;
    		return new URL_with_data(url, crawlDatum);
    	}
    	return null;
    }

    private void skipPage(boolean filterUrl, String urlFilter, boolean filterStatus, byte status)
    		throws InstantiationException, IllegalAccessException, IOException {
    	LOG.info("Skipping page.");
    	int counter = 0;
    	while(counter < PAGE_SIZE && getNext(filterUrl, urlFilter, filterStatus, status) != null) {
    		counter++;
    	}
    }
    
    public Map<String, CrawlDatum> readUrl(int pageIndex, String urlFilter, boolean filterStatus, byte status)
    		throws IOException, InstantiationException, IllegalAccessException {
        Map<String, CrawlDatum> map = new TreeMap<String, CrawlDatum>();
        
        boolean filterUrl = (urlFilter != null) && (!"".equals(urlFilter));
        iterator = this.fReaderSet.iterator();

        if (pageIndex <= currentPage) { // rewind back to the correct point 
        	reset();
        }
        int pagesToSkip = pageIndex - currentPage;
        for(int i=0; i < pagesToSkip; i++) {
           	skipPage(filterUrl, urlFilter, filterStatus, status);
        }
        
    	URL_with_data u;
        int counter = 0;
        while (counter < PAGE_SIZE && (u = getNext(filterUrl, urlFilter, filterStatus, status))!= null) {
            counter++;
            map.put(u.url, u.crawlDatum);
        }
        
        currentPage++;
        return map;
    }

}
